

设全bn层的输入X是一个4维张量，形状是(N, C, H, W)，N代表批大小，C代表属性数；
bn层的权重是S，形状是(C, )；bn层的偏移是B，形状是(C, )；
bn层的输出是Y，形状是(N, C, H, W)。那么有
Y=(X-u)/std * S + B
XW代表矩阵乘法，结果是一个形状为(N, S)的张量，这个张量每一行都加上了B（即B会重复N次）就组成了Y

取特殊值
N=2
C=2
H=2
W=2
e = epsilon
，那么

X = [
[x00, x01],
[x10, x11]]   # (N, C)   x00是一个形状为(H, W)的张量，其余同理。

S = [s0, s1]   # (C, )

B = [b0, b1]   # (C, )

Y = [
[y00, y01],
[y10, y11]]   # (N, C)   y00是一个形状为(H, W)的张量，其余同理。

grad = [
[dloss/dy00, dloss/dy01],
[dloss/dy10, dloss/dy11]]   # (N, C)  dloss/dy00是一个形状为(H, W)的张量，其余同理。

grad是最终loss对本层输出张量Y的梯度，形状和Y一样，是(N, C, H, W)。


训练时前向传播有
u0 = np.sum(x00 + x10) / (N*H*W)   # 不同样本的同一属性（通道）相加
= (x0000 + x1000 + x0001 + x1001 + x0010 + x1010 + x0011 + x1011) / (N*H*W)
u1 = np.sum(x01 + x11) / (N*H*W)   # 不同样本的同一属性（通道）相加
= (x0100 + x1100 + x0101 + x1101 + x0110 + x1110 + x0111 + x1111) / (N*H*W)

v0 = [(x0000-u0)^2 + (x1000-u0)^2 + ... + (x1011-u0)^2] / (N*H*W)
=[x0000^2 + x1000^2 + ... + x1011^2 -2*u0*(x0000 + x1000 + ... + x1011)   + N*H*W*u0^2] / (N*H*W)
=[x0000^2 + x1000^2 + ... + x1011^2 -2*u0*(x0000 + x1000 + ... + x1011)] / (N*H*W) + u0^2
=(x0000^2 + x1000^2 + ... + x1011^2) / (N*H*W) - u0^2
同理
v1 = (x0100^2 + x1100^2 + ... + x1111^2) / (N*H*W) - u1^2


y00 = (x00 - u0) / ((v0 + e)**0.5) * s0 + b0   # y00和x00是形状为(H, W)的张量，其余为标量
= normx00 * s0 + b0   # 真正运算时，标量s0和b0单独重复H*W次变成形状为(H, W)的张量，使得它们可以和normx00逐元素相乘、相加。因为是整体操作，其实这条公式是包含了H*W条公式的。
y01 = (x01 - u1) / ((v1 + e)**0.5) * s1 + b1   # y01和x01是形状为(H, W)的张量，其余为标量
= normx01 * s1 + b1   # 真正运算时，标量s1和b1单独重复H*W次变成形状为(H, W)的张量，使得它们可以和normx01逐元素相乘、相加。因为是整体操作，其实这条公式是包含了H*W条公式的。

y10 = (x10 - u0) / ((v0 + e)**0.5) * s0 + b0   # y10和x10是形状为(H, W)的张量，其余为标量
= normx10 * s0 + b0   # 真正运算时，标量s0和b0单独重复H*W次变成形状为(H, W)的张量，使得它们可以和normx10逐元素相乘、相加。因为是整体操作，其实这条公式是包含了H*W条公式的。
y11 = (x11 - u1) / ((v1 + e)**0.5) * s1 + b1   # y11和x11是形状为(H, W)的张量，其余为标量
= normx11 * s1 + b1   # 真正运算时，标量s1和b1单独重复H*W次变成形状为(H, W)的张量，使得它们可以和normx11逐元素相乘、相加。因为是整体操作，其实这条公式是包含了H*W条公式的。


所以（注意，这里公式的左边代表的是逐元素求偏导，即分子（一个形状为(H, W的张量)）分母（一个形状为(H, W的张量)）相同位置的元素求偏导）
dy00/db0 = 1         # 因为b0重复H*W次变成形状为(H, W)的张量，所以这里的1是一个形状为(H, W)的张量，里面的元素全为1
dy00/ds0 = normx00   # 因为s0重复H*W次变成形状为(H, W)的张量，所以这里的s0是一个形状为(H, W)的张量，里面的元素全为s0
dy00/dnormx00 = s0   # 因为s0重复H*W次变成形状为(H, W)的张量，所以这里的s0是一个形状为(H, W)的张量，里面的元素全为s0
dy01/db1 = 1         # 因为b1重复H*W次变成形状为(H, W)的张量，所以这里的1是一个形状为(H, W)的张量，里面的元素全为1
dy01/ds1 = normx01   # 因为s1重复H*W次变成形状为(H, W)的张量，所以这里的s1是一个形状为(H, W)的张量，里面的元素全为s1
dy01/dnormx01 = s1   # 因为s1重复H*W次变成形状为(H, W)的张量，所以这里的s1是一个形状为(H, W)的张量，里面的元素全为s1

dy10/db0 = 1         # 因为b0重复H*W次变成形状为(H, W)的张量，所以这里的1是一个形状为(H, W)的张量，里面的元素全为1
dy10/ds0 = normx10   # 因为s0重复H*W次变成形状为(H, W)的张量，所以这里的s0是一个形状为(H, W)的张量，里面的元素全为s0
dy10/dnormx10 = s0   # 因为s0重复H*W次变成形状为(H, W)的张量，所以这里的s0是一个形状为(H, W)的张量，里面的元素全为s0
dy11/db1 = 1         # 因为b1重复H*W次变成形状为(H, W)的张量，所以这里的1是一个形状为(H, W)的张量，里面的元素全为1
dy11/ds1 = normx11   # 因为s1重复H*W次变成形状为(H, W)的张量，所以这里的s1是一个形状为(H, W)的张量，里面的元素全为s1
dy11/dnormx11 = s1   # 因为s1重复H*W次变成形状为(H, W)的张量，所以这里的s1是一个形状为(H, W)的张量，里面的元素全为s1

所以
(1)loss对B的梯度。
因为dloss/dy00是一个形状为(H, W)的张量，里面有H*W个偏导数，所以和dy00/db0逐元素相乘后，要求和。其余同理。
dloss/db0 = np.sum(dloss/dy00 * dy00/db0) + np.sum(dloss/dy01 * dy01/db0)
+ np.sum(dloss/dy10 * dy10/db0) + np.sum(dloss/dy11 * dy11/db0)
= np.sum(dloss/dy00 * 1) + np.sum(dloss/dy10 * 1)
= np.sum(dloss/dy00) + np.sum(dloss/dy10)
= np.sum(grad, axis=(0, 2, 3))[0]   # 这个和的第0项是loss对b0的偏导数
同理
dloss/db1 = np.sum(grad, axis=(0, 2, 3))[1]   # 这个和的第1项是loss对b1的偏导数
所以对于整体的B
dloss/dB = np.sum(grad, axis=(0, 2, 3))




(2)loss对S的梯度
dloss/ds0 = np.sum(dloss/dy00 * dy00/ds0) + np.sum(dloss/dy01 * dy01/ds0)
+ np.sum(dloss/dy10 * dy10/ds0) + np.sum(dloss/dy11 * dy11/ds0)
= np.sum(dloss/dy00 * normx00) + np.sum(dloss/dy10 * normx10)
= np.sum(grad[:, 0:1] * normX[:, 0:1])   # grad和normX都取了第0个通道，逐元素相乘后，求总和
同理
dloss/ds1 = np.sum(grad[:, 1:2] * normX[:, 1:2])   # grad和normX都取了第1个通道，逐元素相乘后，求总和

loss对S的梯度，等价于执行
dScale = grad * normX  # 中文记法：等于loss对本层输出的梯度 乘以 本层输入归一化后的值（用的是这一批的均值和方差进行归一化），再对NHW维求和。
dScale = np.sum(dScale, axis=(0, 2, 3))





X = [
[x00, x01],
[x10, x11]]   # (N, C)   x00是一个形状为(H, W)的张量，其余同理。

S = [s0, s1]   # (C, )

B = [b0, b1]   # (C, )

Y = [
[y00, y01],
[y10, y11]]   # (N, C)   y00是一个形状为(H, W)的张量，其余同理。

grad = [
[dloss/dy00, dloss/dy01],
[dloss/dy10, dloss/dy11]]   # (N, C)  dloss/dy00是一个形状为(H, W)的张量，其余同理。


u0 = np.sum(x00 + x10) / (N*H*W)   # 不同样本的同一属性（通道）相加
= (x0000 + x1000 + x0001 + x1001 + x0010 + x1010 + x0011 + x1011) / (N*H*W)
u1 = np.sum(x01 + x11) / (N*H*W)   # 不同样本的同一属性（通道）相加
= (x0100 + x1100 + x0101 + x1101 + x0110 + x1110 + x0111 + x1111) / (N*H*W)

v0 = [(x0000-u0)^2 + (x1000-u0)^2 + ... + (x1011-u0)^2] / (N*H*W)
=[x0000^2 + x1000^2 + ... + x1011^2 -2*u0*(x0000 + x1000 + ... + x1011)   + N*H*W*u0^2] / (N*H*W)
=[x0000^2 + x1000^2 + ... + x1011^2 -2*u0*(x0000 + x1000 + ... + x1011)] / (N*H*W) + u0^2
=(x0000^2 + x1000^2 + ... + x1011^2) / (N*H*W) - u0^2
同理
v1 = (x0100^2 + x1100^2 + ... + x1111^2) / (N*H*W) - u1^2


y00 = (x00 - u0) / ((v0 + e)**0.5) * s0 + b0   # y00和x00是形状为(H, W)的张量，其余为标量
= normx00 * s0 + b0   # 真正运算时，标量s0和b0单独重复H*W次变成形状为(H, W)的张量，使得它们可以和normx00逐元素相乘、相加。因为是整体操作，其实这条公式是包含了H*W条公式的。
y01 = (x01 - u1) / ((v1 + e)**0.5) * s1 + b1   # y01和x01是形状为(H, W)的张量，其余为标量
= normx01 * s1 + b1   # 真正运算时，标量s1和b1单独重复H*W次变成形状为(H, W)的张量，使得它们可以和normx01逐元素相乘、相加。因为是整体操作，其实这条公式是包含了H*W条公式的。

y10 = (x10 - u0) / ((v0 + e)**0.5) * s0 + b0   # y10和x10是形状为(H, W)的张量，其余为标量
= normx10 * s0 + b0   # 真正运算时，标量s0和b0单独重复H*W次变成形状为(H, W)的张量，使得它们可以和normx10逐元素相乘、相加。因为是整体操作，其实这条公式是包含了H*W条公式的。
y11 = (x11 - u1) / ((v1 + e)**0.5) * s1 + b1   # y11和x11是形状为(H, W)的张量，其余为标量
= normx11 * s1 + b1   # 真正运算时，标量s1和b1单独重复H*W次变成形状为(H, W)的张量，使得它们可以和normx11逐元素相乘、相加。因为是整体操作，其实这条公式是包含了H*W条公式的。



dy00/db0 = 1         # 因为b0重复H*W次变成形状为(H, W)的张量，所以这里的1是一个形状为(H, W)的张量，里面的元素全为1
dy00/ds0 = normx00   # 因为s0重复H*W次变成形状为(H, W)的张量，所以这里的s0是一个形状为(H, W)的张量，里面的元素全为s0
dy00/dnormx00 = s0   # 因为s0重复H*W次变成形状为(H, W)的张量，所以这里的s0是一个形状为(H, W)的张量，里面的元素全为s0
dy01/db1 = 1         # 因为b1重复H*W次变成形状为(H, W)的张量，所以这里的1是一个形状为(H, W)的张量，里面的元素全为1
dy01/ds1 = normx01   # 因为s1重复H*W次变成形状为(H, W)的张量，所以这里的s1是一个形状为(H, W)的张量，里面的元素全为s1
dy01/dnormx01 = s1   # 因为s1重复H*W次变成形状为(H, W)的张量，所以这里的s1是一个形状为(H, W)的张量，里面的元素全为s1

dy10/db0 = 1         # 因为b0重复H*W次变成形状为(H, W)的张量，所以这里的1是一个形状为(H, W)的张量，里面的元素全为1
dy10/ds0 = normx10   # 因为s0重复H*W次变成形状为(H, W)的张量，所以这里的s0是一个形状为(H, W)的张量，里面的元素全为s0
dy10/dnormx10 = s0   # 因为s0重复H*W次变成形状为(H, W)的张量，所以这里的s0是一个形状为(H, W)的张量，里面的元素全为s0
dy11/db1 = 1         # 因为b1重复H*W次变成形状为(H, W)的张量，所以这里的1是一个形状为(H, W)的张量，里面的元素全为1
dy11/ds1 = normx11   # 因为s1重复H*W次变成形状为(H, W)的张量，所以这里的s1是一个形状为(H, W)的张量，里面的元素全为s1
dy11/dnormx11 = s1   # 因为s1重复H*W次变成形状为(H, W)的张量，所以这里的s1是一个形状为(H, W)的张量，里面的元素全为s1


(3)loss对X的梯度。
这时候，不同样本之间是有关系的。比如第0个样本bn层的输出不仅和第0个样本的输入有关，还和其它样本的输入有关，因为bn层会求一次不同样本同一通道的均值和方差，
所有样本前向传播时，都会用到这些均值方差。所以
dloss/dx00 =   # (表示了H*W个偏导数)
dloss/dnormx00 * dnormx00/dx00 + dloss/dnormx10 * dnormx10/dx00 + dloss/dnormx01 * dnormx01/dx00 + dloss/dnormx11 * dnormx11/dx00
=dloss/dnormx00 * dnormx00/dx00 + dloss/dnormx10 * dnormx10/dx00    # 因为是loss对X的第0个样本的第0个通道求偏导，和其它通道是无关的。只会留下通道0的偏导数。
（你也可以这样理解，因为x00参与了normx00和normx10的运算（即包含在u0和v0里），但是没有参与normx01和normx11的运算。）
=dloss/dy00 * dy00/dnormx00 * dnormx00/dx00 + dloss/dy10 * dy10/dnormx10 * dnormx10/dx00    # 因为normx00生成了y00，normx10生成了y10
=dloss/dy00 * s0 * dnormx00/dx00 + dloss/dy10 * s0 * dnormx10/dx00

而
normx00 = (x00 - u0) / ((v0 + e)**0.5)
normx10 = (x10 - u0) / ((v0 + e)**0.5)
u0 = (x0000 + x1000 + x0001 + x1001 + x0010 + x1010 + x0011 + x1011) / (N*H*W)
v0 = (x0000^2 + x1000^2 + ... + x1011^2) / (N*H*W) - u0^2

所以        (此处的1是一个(H, W)的张量，因为是整体求导)
dnormx00/dx00 = [ (1 - du0/dx00)*((v0 + e)**0.5) - (x00 - u0) *(dv0/dx00) / (2*((v0 + e)**0.5))    ] / (v0 + e)
dnormx10/dx00 = [ (0 - du0/dx00)*((v0 + e)**0.5) - (x00 - u0) *(dv0/dx00) / (2*((v0 + e)**0.5))    ] / (v0 + e)
du0/dx00 = 1.0 / (N*H*W)   # 是一个(H, W)的张量，里面的元素全是1.0 / (N*H*W)，因为是整体求导。不信的话你试试单独对x00里面的元素x0000、x0001、x0010、x0011求导。
dv0/dx00 = 2.0 / (N*H*W) * x00 - 2 * u0 * du0/dx00      # 是一个(H, W)的张量，里面的元素全是1.0 / (N*H*W)，因为是整体求导
= 2.0 / (N*H*W) * x00 - 2 * u0 * 1.0 / (N*H*W)
= 2.0 / (N*H*W) * (x00 - u0)    # u0是一个标量，会重复H*W次变成形状为(H, W)的张量，和x00逐元素相减




dloss/dx00 = dloss/dy00 * dy00/dx00 + dloss/dy01 * dy01/dx00 + dloss/dy10 * dy10/dx00 + dloss/dy11 * dy11/dx00
= dloss/dy00 * w00 + dloss/dy01 * w01   # 只和loss对第0个样本的输出y00、y01的偏导数有关。和第1个样本无关。你(第1个样本)走你的阳光道，我(第0个样本)过我的独木桥。
dloss/dx01 = dloss/dy00 * dy00/dx01 + dloss/dy01 * dy01/dx01 + dloss/dy10 * dy10/dx01 + dloss/dy11 * dy11/dx01
= dloss/dy00 * w10 + dloss/dy01 * w11   # 只和loss对第0个样本的输出y00、y01的偏导数有关。和第1个样本无关。你(第1个样本)走你的阳光道，我(第0个样本)过我的独木桥。

所以对于第0个样本
dloss/dx0 = [dloss/dx00, dloss/dx01]     # (1, C)
= [dloss/dy00 * w00 + dloss/dy01 * w01, dloss/dy00 * w10 + dloss/dy01 * w11]    # (1, C)

等价于执行
sample0_grad = grad[0:1, :]    #   (1, S)  第0个样本的梯度（loss对本层输出的梯度）
dx0 = sample0_grad * W         #   (C, S)  第0个样本的梯度乘以权重
dx0 = np.sum(dx0, axis=(1, ))  #   (C, )   把偏移数量那一维求和
dx0 = np.expand_dims(dx0, 0)   #   (1, C) 


这一批样本每个样本都是这样求梯度，所以直接归纳为（等价于执行）：
exp_grad = np.expand_dims(grad, 1)   # [N, 1, S]
exp_W = np.expand_dims(W, 0)         # [1, C, S]
dX = exp_grad * exp_W                # [N, C, S]
dX = np.sum(dX, axis=(2, ))          # [N, C]   把偏移数量那一维求和


小结：
N、C、S不取特殊值，直接推导的话也是这些结果。
但是由于兼顾的变量太多，有点劝退读者，故此处选取特殊值。
有兴趣的读者可以不取特殊值直接推导看看。







