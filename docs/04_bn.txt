


设bn层的输入X是一个4维张量，形状是(N, C, H, W)，N代表批大小，C代表属性数；
bn层的权重是S，形状是(C, )；bn层的偏移是B，形状是(C, )；
bn层的输出是Y，形状是(N, C, H, W)。那么有
Y=(X-u)/std * S + B


由于N、H、W维是无差别地相加的。输入X里有N张图片，每张图片的分辨率是H*W，通道数是C。我们不妨把每一张特征图的每一个像素看作为样本，即我们现在把N个样本分解成了N*H*W个样本，它们有C个属性。
X = X.transpose(0, 2, 3, 1)   # (N, H, W, C) 我们先把通道放到最后一维，这样reshape时才能精准拿到通道属性。
N, C, H, W = X.shape
X = X.transpose(0, 2, 3, 1)   # NHWC格式。我们先把通道放到最后一维，这样reshape时才能精准拿到通道属性。
X = np.reshape(X, (-1, C))    # (M, C)
M = N*H*W   # M是新的批大小

这样，我们就把4维张量X变成了2维张量，形状是(M, C)，其中M = N*H*W。

取特殊值
N=2
C=2
H=2
W=1
e = epsilon
，那么

X = [
[x00, x01],
[x10, x11],
[x20, x21],
[x30, x31]]   # (M, C)

S = [s0, s1]   # (C, )

B = [b0, b1]   # (C, )

Y = [
[y00, y01],
[y10, y11],
[y20, y21],
[y30, y31]]   # (M, C)

grad = [
[dloss/dy00, dloss/dy01],
[dloss/dy10, dloss/dy11],
[dloss/dy20, dloss/dy21],
[dloss/dy30, dloss/dy31]]   # (M, C)

grad是最终loss对本层输出张量Y的梯度，形状和Y一样，是(M, C)。


训练时前向传播有
u0 = (x00 + x10 + x20 + x30) / M   # 不同样本的同一属性（通道）相加
u1 = (x01 + x11 + x21 + x31) / M   # 不同样本的同一属性（通道）相加

v0 = [(x00-u0)^2 + (x10-u0)^2 + (x20-u0)^2 + (x30-u0)^2] / M
=[x00^2 + x10^2 + x20^2 + x30^2 -2*u0*(x00 + x10 + x20 + x30)   + M*u0^2] / M
=[x00^2 + x10^2 + x20^2 + x30^2 -2*u0*(x00 + x10 + x20 + x30)] / M + u0^2
=(x00^2 + x10^2 + x20^2 + x30^2) / M - u0^2
同理
v1 = (x01^2 + x11^2 + x21^2 + x31^2) / M - u1^2

前向传播时有
y00 = (x00 - u0) / ((v0 + e)**0.5) * s0 + b0
= normx00 * s0 + b0
y01 = (x01 - u1) / ((v1 + e)**0.5) * s1 + b1
= normx01 * s1 + b1

y10 = (x10 - u0) / ((v0 + e)**0.5) * s0 + b0
= normx10 * s0 + b0
y11 = (x11 - u1) / ((v1 + e)**0.5) * s1 + b1
= normx11 * s1 + b1

...
即（新的第j个属性由第j个均值、第j个方差、第j个Scale、第j个Bias生成）
Y[i, j] = (X[i, j] - U[j]) / ((V[j] + e)**0.5) * S[j] + B[j]
= normX[i, j] * S[j] + B[j]

所以
dY[i, j]/dB[j] = 1
dY[i, j]/dS[j] = normX[i, j]
dY[i, j]/dnormX[i, j] = S[j]



所以
(1)loss对B的梯度。
（B[0]只参与了Y[i, 0]的生成）
dloss/db0 = dloss/dy00 * dy00/db0 + dloss/dy10 * dy10/db0 + dloss/dy20 * dy20/db0 + dloss/dy30 * dy30/db0   # 其它偏导数dloss/dY[i, 1]乘了0就不写了
= dloss/dy00 * 1 + dloss/dy10 * 1 + dloss/dy20 * 1 + dloss/dy30 * 1
= dloss/dy00 + dloss/dy10 + dloss/dy20 + dloss/dy30
= grad第0列求和
= np.sum(grad, axis=(0, ))[0]   # 这个和的第0项是loss对b0的偏导数
同理
dloss/db1 = np.sum(grad, axis=(0, ))[1]   # 这个和的第1项是loss对b1的偏导数
所以对于整体的B
dloss/dB = np.sum(grad, axis=(0, ))



(2)loss对S的梯度
（S[0]只参与了Y[i, 0]的生成）
dloss/ds0 = dloss/dy00 * dy00/ds0 + dloss/dy10 * dy10/ds0 + dloss/dy20 * dy20/ds0 + dloss/dy30 * dy30/ds0   # 其它偏导数dloss/dY[i, 1]乘了0就不写了
= dloss/dy00 * normx00 + dloss/dy10 * normx10 + dloss/dy20 * normx20 + dloss/dy30 * normx30
= grad第0列 点积 normX第0列
= np.sum(grad[:, 0] * normX[:, 0])

同理
（S[1]只参与了Y[i, 1]的生成）
dloss/ds1 = grad第1列 点积 normX第1列
= np.sum(grad[:, 1] * normX[:, 1])

对于整体的dScale，等价于执行
dScale = grad * normX    # (M, C)与(M, C)的张量逐元素相乘
dScale = np.sum(dScale, axis=(0, ))  # (C, )   对C个Scale元素的偏导数





Y[i, j] = (X[i, j] - U[j]) / ((V[j] + e)**0.5) * S[j] + B[j]
= normX[i, j] * S[j] + B[j]

grad = [
[dloss/dy00, dloss/dy01],
[dloss/dy10, dloss/dy11],
[dloss/dy20, dloss/dy21],
[dloss/dy30, dloss/dy31]]   # (M, C)


y00 = (x00 - u0) / ((v0 + e)**0.5) * s0 + b0
= normx00 * s0 + b0
y01 = (x01 - u1) / ((v1 + e)**0.5) * s1 + b1
= normx01 * s1 + b1

y10 = (x10 - u0) / ((v0 + e)**0.5) * s0 + b0
= normx10 * s0 + b0
y11 = (x11 - u1) / ((v1 + e)**0.5) * s1 + b1
= normx11 * s1 + b1


(3)loss对X的梯度。
这时候，不同样本之间是有关系的。比如第0个样本bn层的输出不仅和第0个样本的输入有关，还和其它样本的输入有关，因为bn层会求一次不同样本同一通道的均值和方差，
所有样本前向传播时，都会用到这些均值方差。所以
（x00参与了所有Y[k, 0]的生成，其中k=0, 1, 2, ..., M-1）
dloss/dx00 = dloss/dy00 * dy00/dx00 + dloss/dy10 * dy10/dx00 + dloss/dy20 * dy20/dx00 + dloss/dy30 * dy30/dx00   # 其它偏导数乘了0就不写了
= dloss/dy00 * dy00/dnormx00 * dnormx00/dx00
+ dloss/dy10 * dy10/dnormx10 * dnormx10/dx00
+ dloss/dy20 * dy20/dnormx20 * dnormx20/dx00
+ dloss/dy30 * dy30/dnormx30 * dnormx30/dx00
= dloss/dy00 * s0 * dnormx00/dx00
+ dloss/dy10 * s0 * dnormx10/dx00
+ dloss/dy20 * s0 * dnormx20/dx00
+ dloss/dy30 * s0 * dnormx30/dx00
= s0 * (dloss/dy00 * dnormx00/dx00 + dloss/dy10 * dnormx10/dx00 + dloss/dy20 * dnormx20/dx00 + dloss/dy30 * dnormx30/dx00)
而
normx00 = (x00 - u0) / ((v0 + e)**0.5)
normx10 = (x10 - u0) / ((v0 + e)**0.5)
normx20 = (x20 - u0) / ((v0 + e)**0.5)
normx30 = (x30 - u0) / ((v0 + e)**0.5)
u0 = (x00 + x10 + x20 + x30) / M
v0 = (x00^2 + x10^2 + x20^2 + x30^2) / M - u0^2

所以
dnormx00/dx00 = [ (1 - du0/dx00)*((v0 + e)**0.5) - (x00 - u0) *(dv0/dx00) / (2*((v0 + e)**0.5))    ] / (v0 + e)
dnormx10/dx00 = [ (0 - du0/dx00)*((v0 + e)**0.5) - (x00 - u0) *(dv0/dx00) / (2*((v0 + e)**0.5))    ] / (v0 + e)
              = dnormx00/dx00 - 1.0 / ((v0 + e)**0.5)
dnormx20/dx00 = dnormx00/dx00 - 1.0 / ((v0 + e)**0.5)
dnormx30/dx00 = dnormx00/dx00 - 1.0 / ((v0 + e)**0.5)

du0/dx00 = 1.0 / M
dv0/dx00 = 2.0 / M * x00 - 2 * u0 * du0/dx00
         = 2.0 / M * x00 - 2 * u0 * 1.0 / M
         = 2.0 / M * (x00 - u0)

所以（不能再化简了）
dnormx00/dx00 = [ (1 - 1.0 / M)*((v0 + e)**0.5) - (x00 - u0)^2 * 2.0 / M / (2*((v0 + e)**0.5))    ] / (v0 + e)
              = [ (1 - 1.0 / M)*((v0 + e)**0.5) - (x00 - u0)^2 / M / ((v0 + e)**0.5)    ] / (v0 + e)
dnormx10/dx00 = dnormx00/dx00 - 1.0 / ((v0 + e)**0.5)
dnormx20/dx00 = dnormx00/dx00 - 1.0 / ((v0 + e)**0.5)
dnormx30/dx00 = dnormx00/dx00 - 1.0 / ((v0 + e)**0.5)
dloss/dx00
= s0 * (dloss/dy00 * dnormx00/dx00 + dloss/dy10 * dnormx10/dx00 + dloss/dy20 * dnormx20/dx00 + dloss/dy30 * dnormx30/dx00)



grad = [
[dloss/dy00, dloss/dy01],
[dloss/dy10, dloss/dy11],
[dloss/dy20, dloss/dy21],
[dloss/dy30, dloss/dy31]]   # (M, C)


归纳为
dloss/dX[i, j]
= S[j] * (dloss/dY[0, j] * dnormX[0, j]/dX[i, j] + dloss/dY[1, j] * dnormX[1, j]/dX[i, j] + ... + dloss/dY[M-1, j] * dnormX[M-1, j]/dX[i, j])
= 第j个Scale * (grad第j列 点积 向量F[i, j] )

向量F[i, j]是我本人设置的临时中间变量，它长度同样为M。因为向量F[i, j]不是固定不变的，它和i, j有关，所以带了2个参数i, j。
所以我们可以推测出F是一个3维张量，形状是(M, C, M)

dnormX[0, j]/dX[i, j] = dnormX[i, j]/dX[i, j] - 1.0 / ((V[j] + e)**0.5)
dnormX[1, j]/dX[i, j] = dnormX[i, j]/dX[i, j] - 1.0 / ((V[j] + e)**0.5)
...
dnormX[i, j]/dX[i, j] = [ (1 - 1.0 / M)*((V[j] + e)**0.5) - (X[i, j] - U[j])^2 / M / ((V[j] + e)**0.5)    ] / (V[j] + e)
...
dnormX[M-1, j]/dX[i, j] = dnormX[i, j]/dX[i, j] - 1.0 / ((V[j] + e)**0.5)


等价于执行
sample0_grad = grad[0:1, :]    #   (1, S)  第0个样本的梯度（loss对本层输出的梯度）
dx0 = sample0_grad * W         #   (C, S)  第0个样本的梯度乘以权重
dx0 = np.sum(dx0, axis=(1, ))  #   (C, )   把偏移数量那一维求和
dx0 = np.expand_dims(dx0, 0)   #   (1, C) 


这一批样本每个样本都是这样求梯度，所以直接归纳为（等价于执行）：
F = np.zeros((M, C, M), np.float32)


exp_grad = np.expand_dims(grad, 2)      # [M, C, 1]
exp_S = np.reshape(Scale, (1, -1, 1))   # [1, C, 1]
dX = exp_S * exp_grad * F               # [M, C, M]
dX = np.sum(dX, axis=(2, ))             # [M, C]


小结：
N、C、S不取特殊值，直接推导的话也是这些结果。
但是由于兼顾的变量太多，有点劝退读者，故此处选取特殊值。
有兴趣的读者可以不取特殊值直接推导看看。







